{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print (tf.version)\n",
    "\n",
    "number = tf.Variable(324, tf.int16)\n",
    "\n",
    "print (number.shape)\n",
    "\n",
    "# each tensor represents a partially defined computation that will eventually produce a value. each tensor has datatype and shape - has numbers. scalar is one number\n",
    "\n",
    "rank1_tensor = tf.Variable([\"Test\"], tf.string)\n",
    "rank2_tensor = tf.Variable([[\"test\", \"ok\", \"abc\"], [\"test\", \"yes\", 'abc']], tf.string)\n",
    "\n",
    "tf.rank(rank2_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank2_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = tf.ones([1,2,3])\n",
    "tensor2 = tf.reshape(tensor1, [2, 3, 1])\n",
    "tensor3 = tf.reshape(tensor2, [3, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. 1. 1.]\n",
      "  [1. 1. 1.]]], shape=(1, 2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]]], shape=(2, 3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]], shape=(3, 2), dtype=float32)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1357480dff79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtensor3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensor' is not defined"
     ]
    }
   ],
   "source": [
    "print (tensor1)\n",
    "print (tensor2)\n",
    "print (tensor3)\n",
    "tensor3.shape\n",
    "\n",
    "with tensor.Session as sess:\n",
    "    tensor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]], shape=(5, 5, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t = tf.zeros([5, 5, 5, 5])\n",
    "print (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]], shape=(125, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t = tf.reshape(t, [125, -1])\n",
    "print (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x64650b8d0>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# core ML algos - linear regression, classification, clustering, hidden markov models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2, 4, 5.4, 7.2, 9]\n",
    "plt.plot(x, y, \"ro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " !pip install -q sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  Label\n",
      "0       7430.14    9529.78   -2453.33         19        123        621      0\n",
      "1      11256.40   50455.10   -4220.00         18        216       2677      0\n",
      "2      13093.00   51897.10   -2880.00         30        234       2464      0\n",
      "3      14303.00  102632.00   -5702.20        144        281       4061      1\n",
      "4      14688.00   83343.40   -2430.00         52        223       2822      1\n",
      "...         ...        ...        ...        ...        ...        ...    ...\n",
      "5595   10424.80   35041.40   -4345.67        114        208       1691      0\n",
      "5596    5976.87   10806.00   -2662.00          3         98        654      1\n",
      "5597   15793.50  216990.00   -9819.00         44        338       8323      1\n",
      "5598    8787.16   20778.60   -3470.08         92        163       1162      0\n",
      "5599    8322.40   12208.90   -2592.59         83        128        693      0\n",
      "\n",
      "[5600 rows x 7 columns]\n",
      "      Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  Label\n",
      "4337    7513.77    9534.94   -2606.67         18        123        621      0\n",
      "3552    7546.26    4593.67   -2581.74         30        131        271      0\n",
      "1472    9615.02   25822.50   -3391.89         25        174       1415      0\n",
      "4470    8342.16    9418.73   -2786.40         86        130        490      0\n",
      "1402    7029.83   11699.00   -4529.00         48        122        694      1\n",
      "...         ...        ...        ...        ...        ...        ...    ...\n",
      "3028   12894.80   50972.10   -5290.00         34        234       2464      0\n",
      "1583    9520.17   27328.40   -2957.01         99        158       1403      0\n",
      "1435    9956.07   10314.00   -1120.00         38        154        587      0\n",
      "2873   11109.60   35082.00   -8347.00        156        277       1586      1\n",
      "1009    6791.56   10569.00   -1259.00         19        132        586      1\n",
      "\n",
      "[4480 rows x 7 columns]\n",
      "      Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  Label\n",
      "67      9606.79   34330.50   -3513.28         83        176       1922      0\n",
      "1333   30240.50  543358.00  -14115.00        114        708      16061      1\n",
      "3393   15990.70  162381.00   -7684.56         74        352       6744      0\n",
      "2861    6094.49    8143.00   -2117.00         12         87        485      1\n",
      "3783    7127.44    5406.87   -1810.00          8         79        292      0\n",
      "...         ...        ...        ...        ...        ...        ...    ...\n",
      "3663    7087.98   14035.10   -2394.23         84        120        809      0\n",
      "3226    6766.67   13895.00   -2519.00         23        132        847      1\n",
      "870    11050.20   47893.00   -6782.00         31        213       2255      1\n",
      "2434    5589.71    4245.66   -1492.50        123         88        318      0\n",
      "1078    6904.21    6044.00   -2426.00         19        100        249      1\n",
      "\n",
      "[1120 rows x 7 columns]\n",
      "4337    0\n",
      "3552    0\n",
      "1472    0\n",
      "4470    0\n",
      "1402    1\n",
      "       ..\n",
      "3028    0\n",
      "1583    0\n",
      "1435    0\n",
      "2873    1\n",
      "1009    1\n",
      "Name: Label, Length: 4480, dtype: int64\n",
      "(4480, 6)\n",
      "[NumericColumn(key='Feature_1', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='Feature_2', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='Feature_3', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='Feature_4', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='Feature_5', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='Feature_6', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/kg/xgv060d536z6lmls6nlhzrc80000gn/T/tmpks3wluy8\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/kg/xgv060d536z6lmls6nlhzrc80000gn/T/tmpks3wluy8', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/kg/xgv060d536z6lmls6nlhzrc80000gn/T/tmpks3wluy8/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 0.6931472, step = 0\n",
      "INFO:tensorflow:global_step/sec: 495.491\n",
      "INFO:tensorflow:loss = 89.51287, step = 100 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 739.35\n",
      "INFO:tensorflow:loss = 285.93243, step = 200 (0.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 757.082\n",
      "INFO:tensorflow:loss = 351.36792, step = 300 (0.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 772.524\n",
      "INFO:tensorflow:loss = 290.9115, step = 400 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 773.526\n",
      "INFO:tensorflow:loss = 109.69185, step = 500 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 805.898\n",
      "INFO:tensorflow:loss = 44.15088, step = 600 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 769.179\n",
      "INFO:tensorflow:loss = 94.8151, step = 700 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 767.889\n",
      "INFO:tensorflow:loss = 147.63249, step = 800 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 646.112\n",
      "INFO:tensorflow:loss = 78.64655, step = 900 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 774.264\n",
      "INFO:tensorflow:loss = 414.9385, step = 1000 (0.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 710.408\n",
      "INFO:tensorflow:loss = 112.46816, step = 1100 (0.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 700.423\n",
      "INFO:tensorflow:loss = 34.56403, step = 1200 (0.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 755.845\n",
      "INFO:tensorflow:loss = 42.399384, step = 1300 (0.132 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1400...\n",
      "INFO:tensorflow:Saving checkpoints for 1400 into /var/folders/kg/xgv060d536z6lmls6nlhzrc80000gn/T/tmpks3wluy8/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1400...\n",
      "INFO:tensorflow:Loss for final step: 153.75198.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-18ed4969abd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mlinear_est\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_est\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m   def _actual_eval(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_actual_eval\u001b[0;34m(self, input_fn, strategy, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_convert_eval_steps_to_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    490\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         (scaffold, update_op, eval_dict, all_hooks) = (\n\u001b[0;32m--> 492\u001b[0;31m             self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\n\u001b[0m\u001b[1;32m    493\u001b[0m         return self._evaluate_run(\n\u001b[1;32m    494\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate_build_graph\u001b[0;34m(self, input_fn, hooks, checkpoint_path)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m       (scaffold, evaluation_hooks, input_hooks, update_op, eval_dict) = (\n\u001b[0;32m-> 1528\u001b[0;31m           self._call_model_fn_eval(input_fn, self.config))\n\u001b[0m\u001b[1;32m   1529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m     global_step_tensor = tf.compat.v1.train.get_global_step(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn_eval\u001b[0;34m(self, input_fn, config)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     \u001b[0;34m\"\"\"Call model_fn for evaluation and handle return values.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m     features, labels, input_hooks = self._get_features_and_labels_from_input_fn(\n\u001b[0;32m-> 1561\u001b[0;31m         input_fn, ModeKeys.EVAL)\n\u001b[0m\u001b[1;32m   1562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m     estimator_spec = self._call_model_fn(features, labels, ModeKeys.EVAL,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_get_features_and_labels_from_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;34m\"\"\"Extracts the `features` and labels from return values of `input_fn`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     return estimator_util.parse_input_fn_result(\n\u001b[0;32m-> 1037\u001b[0;31m         self._call_input_fn(input_fn, mode))\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extract_batch_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_evaluated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[0;34m(self, input_fn, mode, input_context)\u001b[0m\n\u001b[1;32m   1128\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_context'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-18ed4969abd0>\u001b[0m in \u001b[0;36minput_function\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minput_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not iterable"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v2.feature_column as fc\n",
    "data =pd.read_csv('hw4_trainingset.csv')\n",
    "print (data)\n",
    "# Importing SKlearn's  package \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# this splits the data \n",
    "train, test = train_test_split(data, test_size = .2)\n",
    "print (train)\n",
    "print (test)\n",
    "y_train = train.pop('Label')\n",
    "print (y_train)\n",
    "y_eval = test.pop(\"Label\")\n",
    "print (train.shape)\n",
    "train.describe()\n",
    "train.Feature_1.hist(bins = 20)\n",
    "\n",
    "# things\n",
    "\n",
    "feature_columns = []\n",
    "NUMERIC_COLUMNS = ['Feature_1', \"Feature_2\", \"Feature_3\",'Feature_4', \"Feature_5\", \"Feature_6\"]\n",
    "for feature_name in NUMERIC_COLUMNS:\n",
    "  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
    "\n",
    "print(feature_columns)\n",
    "\n",
    "\n",
    "\n",
    "# training computer suppose you have crazy dataset, laod into batches, give 32 entries at once to the model\n",
    "# epoch - pass data to model, pretty bad, feed data again in diff order, model look at it in diff way\n",
    "\n",
    "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
    "  def input_function():\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n",
    "    if shuffle:\n",
    "      ds = ds.shuffle(1000)\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "    return ds\n",
    "  return input_function\n",
    "\n",
    "train_input_fn = make_input_fn(train, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model\n",
    "eval_input_fn = make_input_fn(eval, y_eval, num_epochs=1, shuffle=False)\n",
    "\n",
    "linear_est = tf.estimator.LinearClassifier(feature_columns = feature_columns)\n",
    "linear_est.train(train_input_fn)\n",
    "result = linear_est.evaluate(eval_input_fn)\n",
    "\n",
    "print (result['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input layer is data we have, then take outputs as inputs for another layer, another\n",
    "### until we decide to stop, last we build is output layer, compare targets to!\n",
    "### first layer is input layer, last layer is output layer, layers between are hidden layers\n",
    "### each hidden unit is element of tensor, number of nodes in layer is referred to as width of layer\n",
    "### depth refers to number of hidden layers in a network, when we create an ML algo pick width/depth as hyperparameters\n",
    "### parameters are weights/biases, h yperparameters is width/depth/learning rate\n",
    "### hyperparamaters preset\n",
    "### combine inputs linearly, then apply them non-linearily\n",
    "### non-linearity doesn't change shape of expression, if 8 input layers predict 9, 72 weights, 72 arrows!\n",
    "\n",
    "\n",
    "### prepreocessing applies to dataset through model\n",
    "\n",
    "# feature scaling/normalization\n",
    "\n",
    "# MNIST DATASET - 70k handwritten digits, have 10 digits, 10 classes (0-9)\n",
    "\n",
    "# takes as input, determines which number is shown!\n",
    "\n",
    "# MNIST is the hello world of machine learning, each image is 28-28 pixels, 28x28 matrix where input values are 0--?255\n",
    "# 0 is black, 255 is white, 784 pixels, flatten - put in 784 inputs!\n",
    "# 2 hidden layers, 10 classes, 10 output units!\n",
    "# then compared to targets, use one-hot encoding for vectors...\n",
    "# use softmax activation function for the layer\n",
    "\n",
    "## deep neural network for MNIST\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# DATA\n",
    "\n",
    "mnist_dataset, mnist_info = tfds.load(name=\"mnist\", with_info=True, as_supervised=True)\n",
    "\n",
    "# split train/test\n",
    "mnist_train, mnist_test = mnist_dataset[\"train\"], mnist_dataset[\"test\"]\n",
    "# where is validation?\n",
    "num_validation_samples = 0.1 * mnist_info.splits[\"train\"].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "# scale data in some way to make result mroe numerically stable\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image / 255\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "# shuffle data, get valiation dataset\n",
    "\n",
    "BUFFER_SIZE = 10000 #dealing w/ enormous datasets, cannot shuffle all at once\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "\n",
    "# Outline model\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "                            ])\n",
    "# choose optimizer and the loss function\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# training\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data = (validation_inputs, validation_targets), verbose=2)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    return image / 255, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "# shuffle data, get valiation dataset\n",
    "\n",
    "BUFFER_SIZE = 10000 #dealing w/ enormous datasets, cannot shuffle all at once\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "\n",
    "# Outline model\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "                            ])\n",
    "# choose optimizer and the loss function\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# training\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data = (validation_inputs, validation_targets), verbose=2)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
